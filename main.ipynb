{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wanted-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No personal conf_private.py found.\n",
      "doodad not detected\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-02-23 23:47:27.223781 UTC | Variant:\n",
      "2021-02-23 23:47:27.224314 UTC | {\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"num_eval_steps_per_epoch\": 5000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"num_epochs\": 1,\n",
      "    \"num_trains_per_train_loop\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"batch_size\": 256\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"reward_scale\": 1,\n",
      "    \"policy_lr\": 0.0003,\n",
      "    \"discount\": 0.99,\n",
      "    \"qf_lr\": 0.0003,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"use_automatic_entropy_tuning\": true,\n",
      "    \"target_update_period\": 1\n",
      "  },\n",
      "  \"version\": \"normal\",\n",
      "  \"layer_size\": 256,\n",
      "  \"algorithm\": \"DIAYN\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210223 23:47:27 train:47] args: Namespace(B=64, H=100, T=10, a_dim=6, data_dir='./data/', depth=0, device=[0], epochs=1, h_dim=128, iters_to_accumulate=1, load_epoch=None, o_dim=17, s_dim=64, seed=0, timestamp='Feb23_23-47-27')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 100, 6) (900, 100, 17)\n",
      "(100, 100, 6) (100, 100, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210223 23:47:29 trainer:53] (train) Epoch: 1 {'x_loss': 245.81510271344865, 'loss': 31405331.598214287, 's_aux_loss': 35.18444102151053, 's_loss': 31405086.42410714}\n",
      "[I 210223 23:47:29 trainer:53] (test) Epoch: 1 {'x_loss': 209.17181396484375, 'loss': 236711.1875, 's_aux_loss': 30.627729415893555, 's_loss': 236502.015625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-02-23 23:47:52.467997 UTC | [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_27_0000--s-0] Epoch 0 finished\n",
      "------------------------------------------  --------------\n",
      "replay_buffer/size                          2000\n",
      "trainer/Intrinsic Rewards                     -0.0042671\n",
      "trainer/DF Loss                                4.60944\n",
      "trainer/DF Accuracy                            0\n",
      "trainer/QF1 Loss                              15.6222\n",
      "trainer/QF2 Loss                              15.6948\n",
      "trainer/Policy Loss                           -4.02072\n",
      "trainer/Q1 Predictions Mean                    0.00414251\n",
      "trainer/Q1 Predictions Std                     0.000796448\n",
      "trainer/Q1 Predictions Max                     0.00619714\n",
      "trainer/Q1 Predictions Min                     0.00261055\n",
      "trainer/Q2 Predictions Mean                   -0.00516464\n",
      "trainer/Q2 Predictions Std                     0.000520687\n",
      "trainer/Q2 Predictions Max                    -0.00391835\n",
      "trainer/Q2 Predictions Min                    -0.00637521\n",
      "trainer/D Predictions Mean                    59\n",
      "trainer/D Predictions Std                      0\n",
      "trainer/D Predictions Max                     59\n",
      "trainer/D Predictions Min                     59\n",
      "trainer/Q Targets Mean                         3.90314\n",
      "trainer/Q Targets Std                          0.647897\n",
      "trainer/Q Targets Max                          5.86693\n",
      "trainer/Q Targets Min                         -0.00425339\n",
      "trainer/Log Pis Mean                          -4.02588\n",
      "trainer/Log Pis Std                            0.538614\n",
      "trainer/Log Pis Max                           -2.60545\n",
      "trainer/Log Pis Min                           -5.35783\n",
      "trainer/Policy mu Mean                        -0.000609165\n",
      "trainer/Policy mu Std                          0.000753663\n",
      "trainer/Policy mu Max                          0.000437716\n",
      "trainer/Policy mu Min                         -0.00191443\n",
      "trainer/Policy log std Mean                   -0.00068527\n",
      "trainer/Policy log std Std                     0.000977373\n",
      "trainer/Policy log std Max                     0.000256656\n",
      "trainer/Policy log std Min                    -0.00260286\n",
      "trainer/Alpha                                  0.9997\n",
      "trainer/Alpha Loss                            -0\n",
      "exploration/num steps total                 2000\n",
      "exploration/num paths total                    2\n",
      "exploration/path length Mean                1000\n",
      "exploration/path length Std                    0\n",
      "exploration/path length Max                 1000\n",
      "exploration/path length Min                 1000\n",
      "exploration/Rewards Mean                       0\n",
      "exploration/Rewards Std                        0\n",
      "exploration/Rewards Max                        0\n",
      "exploration/Rewards Min                        0\n",
      "exploration/Returns Mean                       0\n",
      "exploration/Returns Std                        0\n",
      "exploration/Returns Max                        0\n",
      "exploration/Returns Min                        0\n",
      "exploration/Actions Mean                      -0.00190443\n",
      "exploration/Actions Std                        0.591234\n",
      "exploration/Actions Max                        0.998575\n",
      "exploration/Actions Min                       -0.995066\n",
      "exploration/Num Paths                          1\n",
      "exploration/Average Returns                    0\n",
      "exploration/agent_infos/final/skill Mean       0.01\n",
      "exploration/agent_infos/final/skill Std        0.0994987\n",
      "exploration/agent_infos/final/skill Max        1\n",
      "exploration/agent_infos/final/skill Min        0\n",
      "exploration/agent_infos/initial/skill Mean     0.01\n",
      "exploration/agent_infos/initial/skill Std      0.0994987\n",
      "exploration/agent_infos/initial/skill Max      1\n",
      "exploration/agent_infos/initial/skill Min      0\n",
      "exploration/agent_infos/skill Mean             0.01\n",
      "exploration/agent_infos/skill Std              0.0994987\n",
      "exploration/agent_infos/skill Max              1\n",
      "exploration/agent_infos/skill Min              0\n",
      "evaluation/num steps total                  5000\n",
      "evaluation/num paths total                     5\n",
      "evaluation/path length Mean                 1000\n",
      "evaluation/path length Std                     0\n",
      "evaluation/path length Max                  1000\n",
      "evaluation/path length Min                  1000\n",
      "evaluation/Rewards Mean                        0\n",
      "evaluation/Rewards Std                         0\n",
      "evaluation/Rewards Max                         0\n",
      "evaluation/Rewards Min                         0\n",
      "evaluation/Returns Mean                        0\n",
      "evaluation/Returns Std                         0\n",
      "evaluation/Returns Max                         0\n",
      "evaluation/Returns Min                         0\n",
      "evaluation/Actions Mean                       -0.000705104\n",
      "evaluation/Actions Std                         0.000921962\n",
      "evaluation/Actions Max                         0.000700944\n",
      "evaluation/Actions Min                        -0.00258579\n",
      "evaluation/Num Paths                           5\n",
      "evaluation/Average Returns                     0\n",
      "evaluation/agent_infos/final/skill Mean        0.01\n",
      "evaluation/agent_infos/final/skill Std         0.0994987\n",
      "evaluation/agent_infos/final/skill Max         1\n",
      "evaluation/agent_infos/final/skill Min         0\n",
      "evaluation/agent_infos/initial/skill Mean      0.01\n",
      "evaluation/agent_infos/initial/skill Std       0.0994987\n",
      "evaluation/agent_infos/initial/skill Max       1\n",
      "evaluation/agent_infos/initial/skill Min       0\n",
      "evaluation/agent_infos/skill Mean              0.01\n",
      "evaluation/agent_infos/skill Std               0.0994987\n",
      "evaluation/agent_infos/skill Max               1\n",
      "evaluation/agent_infos/skill Min               0\n",
      "time/data storing (s)                          0.00763101\n",
      "time/evaluation sampling (s)                   4.7569\n",
      "time/exploration sampling (s)                  1.67354\n",
      "time/logging (s)                               0.0187593\n",
      "time/saving (s)                                0.00561891\n",
      "time/training (s)                             15.5295\n",
      "time/epoch (s)                                21.992\n",
      "time/total (s)                                25.5877\n",
      "Epoch                                          0\n",
      "------------------------------------------  --------------\n",
      "2021-02-23 23:47:52.563947 UTC | [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_27_0000--s-0] Variant:\n",
      "2021-02-23 23:47:52.564862 UTC | [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_27_0000--s-0] {\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"num_eval_steps_per_epoch\": 5000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"num_epochs\": 1,\n",
      "    \"num_trains_per_train_loop\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"batch_size\": 256\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"reward_scale\": 1,\n",
      "    \"policy_lr\": 0.0003,\n",
      "    \"discount\": 0.99,\n",
      "    \"qf_lr\": 0.0003,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"use_automatic_entropy_tuning\": true,\n",
      "    \"target_update_period\": 1\n",
      "  },\n",
      "  \"version\": \"normal\",\n",
      "  \"layer_size\": 256,\n",
      "  \"algorithm\": \"DIAYN\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210223 23:47:52 train:47] args: Namespace(B=64, H=100, T=10, a_dim=6, data_dir='./data/', depth=1, device=[0], epochs=1, h_dim=128, iters_to_accumulate=1, load_epoch=None, o_dim=17, s_dim=64, seed=0, timestamp='Feb23_23-47-52')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 100, 6) (900, 100, 17)\n",
      "(100, 100, 6) (100, 100, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210223 23:47:53 trainer:53] (train) Epoch: 1 {'x_loss': 238.6244386945452, 'loss': 27548396.651785713, 's_aux_loss': 41.14420182364328, 's_loss': 27548157.36495536}\n",
      "[I 210223 23:47:53 trainer:53] (test) Epoch: 1 {'x_loss': 216.10598754882812, 'loss': 188829.890625, 's_aux_loss': 36.294288635253906, 's_loss': 188613.78125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-02-23 23:48:16.932403 UTC | [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_27_0000--s-0] [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_52_0000--s-0] Epoch 0 finished\n",
      "------------------------------------------  --------------\n",
      "replay_buffer/size                          2000\n",
      "trainer/Intrinsic Rewards                    -11.573\n",
      "trainer/DF Loss                               16.1781\n",
      "trainer/DF Accuracy                            0\n",
      "trainer/QF1 Loss                              93.0057\n",
      "trainer/QF2 Loss                              80.0507\n",
      "trainer/Policy Loss                          -30.6685\n",
      "trainer/Q1 Predictions Mean                   27.2991\n",
      "trainer/Q1 Predictions Std                     2.99603\n",
      "trainer/Q1 Predictions Max                    32.3188\n",
      "trainer/Q1 Predictions Min                    21.258\n",
      "trainer/Q2 Predictions Mean                   26.5708\n",
      "trainer/Q2 Predictions Std                     2.90656\n",
      "trainer/Q2 Predictions Max                    31.3902\n",
      "trainer/Q2 Predictions Min                    20.7986\n",
      "trainer/D Predictions Mean                     0\n",
      "trainer/D Predictions Std                      0\n",
      "trainer/D Predictions Max                      0\n",
      "trainer/D Predictions Min                      0\n",
      "trainer/Q Targets Mean                        18.3428\n",
      "trainer/Q Targets Std                          2.94808\n",
      "trainer/Q Targets Max                         21.7691\n",
      "trainer/Q Targets Min                        -11.2811\n",
      "trainer/Log Pis Mean                          -4.09061\n",
      "trainer/Log Pis Std                            0.368597\n",
      "trainer/Log Pis Max                           -3.20605\n",
      "trainer/Log Pis Min                           -6.22057\n",
      "trainer/Policy mu Mean                        -0.000509145\n",
      "trainer/Policy mu Std                          0.0301564\n",
      "trainer/Policy mu Max                          0.0435959\n",
      "trainer/Policy mu Min                         -0.0498782\n",
      "trainer/Policy log std Mean                   -0.10735\n",
      "trainer/Policy log std Std                     0.00720221\n",
      "trainer/Policy log std Max                    -0.0903984\n",
      "trainer/Policy log std Min                    -0.128012\n",
      "trainer/Alpha                                  0.9997\n",
      "trainer/Alpha Loss                            -0\n",
      "exploration/num steps total                 2000\n",
      "exploration/num paths total                    2\n",
      "exploration/path length Mean                1000\n",
      "exploration/path length Std                    0\n",
      "exploration/path length Max                 1000\n",
      "exploration/path length Min                 1000\n",
      "exploration/Rewards Mean                       0\n",
      "exploration/Rewards Std                        0\n",
      "exploration/Rewards Max                        0\n",
      "exploration/Rewards Min                        0\n",
      "exploration/Returns Mean                       0\n",
      "exploration/Returns Std                        0\n",
      "exploration/Returns Max                        0\n",
      "exploration/Returns Min                        0\n",
      "exploration/Actions Mean                      -0.0764038\n",
      "exploration/Actions Std                        0.639376\n",
      "exploration/Actions Max                        0.998952\n",
      "exploration/Actions Min                       -0.997131\n",
      "exploration/Num Paths                          1\n",
      "exploration/Average Returns                    0\n",
      "exploration/agent_infos/final/skill Mean       0.01\n",
      "exploration/agent_infos/final/skill Std        0.0994987\n",
      "exploration/agent_infos/final/skill Max        1\n",
      "exploration/agent_infos/final/skill Min        0\n",
      "exploration/agent_infos/initial/skill Mean     0.01\n",
      "exploration/agent_infos/initial/skill Std      0.0994987\n",
      "exploration/agent_infos/initial/skill Max      1\n",
      "exploration/agent_infos/initial/skill Min      0\n",
      "exploration/agent_infos/skill Mean             0.01\n",
      "exploration/agent_infos/skill Std              0.0994987\n",
      "exploration/agent_infos/skill Max              1\n",
      "exploration/agent_infos/skill Min              0\n",
      "evaluation/num steps total                  5000\n",
      "evaluation/num paths total                     5\n",
      "evaluation/path length Mean                 1000\n",
      "evaluation/path length Std                     0\n",
      "evaluation/path length Max                  1000\n",
      "evaluation/path length Min                  1000\n",
      "evaluation/Rewards Mean                        0\n",
      "evaluation/Rewards Std                         0\n",
      "evaluation/Rewards Max                         0\n",
      "evaluation/Rewards Min                         0\n",
      "evaluation/Returns Mean                        0\n",
      "evaluation/Returns Std                         0\n",
      "evaluation/Returns Max                         0\n",
      "evaluation/Returns Min                         0\n",
      "evaluation/Actions Mean                       -0.000529749\n",
      "evaluation/Actions Std                         0.0308402\n",
      "evaluation/Actions Max                         0.0455463\n",
      "evaluation/Actions Min                        -0.0522843\n",
      "evaluation/Num Paths                           5\n",
      "evaluation/Average Returns                     0\n",
      "evaluation/agent_infos/final/skill Mean        0.01\n",
      "evaluation/agent_infos/final/skill Std         0.0994987\n",
      "evaluation/agent_infos/final/skill Max         1\n",
      "evaluation/agent_infos/final/skill Min         0\n",
      "evaluation/agent_infos/initial/skill Mean      0.01\n",
      "evaluation/agent_infos/initial/skill Std       0.0994987\n",
      "evaluation/agent_infos/initial/skill Max       1\n",
      "evaluation/agent_infos/initial/skill Min       0\n",
      "evaluation/agent_infos/skill Mean              0.01\n",
      "evaluation/agent_infos/skill Std               0.0994987\n",
      "evaluation/agent_infos/skill Max               1\n",
      "evaluation/agent_infos/skill Min               0\n",
      "time/data storing (s)                          0.00768258\n",
      "time/evaluation sampling (s)                   4.88884\n",
      "time/exploration sampling (s)                  1.70312\n",
      "time/logging (s)                               0.0251299\n",
      "time/saving (s)                                0.00575865\n",
      "time/training (s)                             15.7413\n",
      "time/epoch (s)                                22.3718\n",
      "time/total (s)                                24.3679\n",
      "Epoch                                          0\n",
      "------------------------------------------  --------------\n",
      "2021-02-23 23:48:17.021759 UTC | [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_27_0000--s-0] [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_52_0000--s-0] Variant:\n",
      "2021-02-23 23:48:17.022461 UTC | [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_27_0000--s-0] [DIAYN_100_HalfCheetah-v2_2021_02_23_23_47_52_0000--s-0] {\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"num_eval_steps_per_epoch\": 5000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"num_epochs\": 1,\n",
      "    \"num_trains_per_train_loop\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"batch_size\": 256\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"reward_scale\": 1,\n",
      "    \"policy_lr\": 0.0003,\n",
      "    \"discount\": 0.99,\n",
      "    \"qf_lr\": 0.0003,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"use_automatic_entropy_tuning\": true,\n",
      "    \"target_update_period\": 1\n",
      "  },\n",
      "  \"version\": \"normal\",\n",
      "  \"layer_size\": 256,\n",
      "  \"algorithm\": \"DIAYN\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import pickle\n",
    "import gtimer as gt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"./dssm\")\n",
    "from train import main as train_ssm\n",
    "from diayn.examples.diayn import get_algorithm, get_algorithm_resume, experiment\n",
    "from rlkit.envs.wrappers import NormalizedBoxEnv\n",
    "from rlkit.samplers.util import DIAYNRollout as rollout\n",
    "\n",
    "\n",
    "def collect(env, diayn, depth, args):\n",
    "    policy = diayn.eval_data_collector.get_snapshot()['policy']\n",
    "\n",
    "    data = []\n",
    "    for skill in tqdm(range(policy.stochastic_policy.skill_dim)):\n",
    "        for trial in range(10):\n",
    "            # print(\"skill-{} rollout-{}\".format(skill, trial))\n",
    "            path = rollout(\n",
    "                env,\n",
    "                policy,\n",
    "                skill,\n",
    "                max_path_length=args.H,\n",
    "                render=False,\n",
    "            )\n",
    "            data.append([path['actions'], path['next_observations']])\n",
    "\n",
    "    train_data = data[:int(len(data)*0.9)]\n",
    "    test_data = data[int(len(data)*0.9):]\n",
    "\n",
    "    train_path = os.path.join(args.data_dir, \"./train{}.pkl\".format(depth))\n",
    "    test_path = os.path.join(args.data_dir, \"./test{}.pkl\".format(depth))\n",
    "    os.makedirs(args.data_dir, exist_ok=True)\n",
    "    with open(train_path, mode='wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(test_path, mode='wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "\n",
    "\n",
    "def update_sim(env, depth, args):\n",
    "    ssm = train_ssm(\"--H {} --depth {} --epochs 1\".format(args.H, depth))\n",
    "    sim = SimNormalizedBoxEnv(env, ssm, depth, args)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def update_policy(diayn, sim, log_dir, args):\n",
    "    experiment(diayn, sim, sim, args)\n",
    "    file = os.path.join(log_dir, \"params.pkl\")\n",
    "    diayn, log_dir = get_algorithm_resume(env, env, args.skill_dim, file)\n",
    "    return diayn, log_dir\n",
    "\n",
    "\n",
    "class SimNormalizedBoxEnv(NormalizedBoxEnv):\n",
    "    def __init__(self, env, ssm, depth, args):\n",
    "        super(SimNormalizedBoxEnv, self).__init__(env)\n",
    "        self.ssm = ssm\n",
    "        with open(os.path.join(args.data_dir, \"param{}.pkl\".format(depth)),\n",
    "                  mode='rb') as f:\n",
    "            self.a_mean, self.a_std, self.o_mean, self.o_std = \\\n",
    "                pickle.load(f)\n",
    "        self.env.step = self.step\n",
    "        self.envreset = self.env.reset\n",
    "        self.env.reset = self.reset\n",
    "\n",
    "    def step(self, action):\n",
    "        lb = self._wrapped_env.action_space.low\n",
    "        ub = self._wrapped_env.action_space.high\n",
    "        scaled_action = lb + (action + 1.) * 0.5 * (ub - lb)\n",
    "        scaled_action = np.clip(scaled_action, lb, ub)\n",
    "\n",
    "        # wrapped_step = self._wrapped_env.step(scaled_action)\n",
    "        # next_obs, reward, done, info = wrapped_step\n",
    "        # if self._should_normalize:\n",
    "        #     next_obs = self._apply_normalize_obs(next_obs)\n",
    "\n",
    "        a = scaled_action.astype(np.float32)\n",
    "        a = (a - self.a_mean) / self.a_std\n",
    "        a = torch.from_numpy(np.array([a]))\n",
    "        o = self.ssm.step(a)[0]\n",
    "        o = o.cpu().detach().numpy()\n",
    "        next_obs = o * self.o_std + self.o_mean\n",
    "\n",
    "        # return next_obs, reward * self._reward_scale, done, info\n",
    "        return next_obs, 0, False, {}\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        o_original = self.envreset(**kwargs)\n",
    "        o = o_original.astype(np.float32)\n",
    "        o = (o - self.o_mean) / self.o_std\n",
    "        o = torch.from_numpy(np.array([o]))\n",
    "        o = self.ssm.reset(o)\n",
    "        return o_original\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('env', type=str,\n",
    "                        help='environment')\n",
    "    parser.add_argument(\"--data_dir\", type=str,\n",
    "                        default=\"./data/\")\n",
    "    parser.add_argument('--skill_dim', type=int, default=100,\n",
    "                        help='skill dimension')\n",
    "    parser.add_argument('--H', type=int, default=300,\n",
    "                        help='Max length of rollout')\n",
    "    parser.add_argument('--D', type=int, default=2,\n",
    "                        help='Depth (The number of update)')\n",
    "    args = parser.parse_args(\"HalfCheetah-v2 --H 100\".split())\n",
    "\n",
    "    env = NormalizedBoxEnv(gym.make(str(args.env)))\n",
    "    sim = None\n",
    "    diayn, log_dir = get_algorithm(env, env, args.skill_dim)\n",
    "\n",
    "    for depth in range(args.D):\n",
    "        # collect(env, diayn, depth, args)  # Sim class test\n",
    "        sim = update_sim(env, depth, args)\n",
    "        diayn, log_dir = update_policy(diayn, sim, log_dir, args)\n",
    "        gt.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-planning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
