{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "superb-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import pickle\n",
    "import gtimer as gt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"./dssm\")\n",
    "from train import main as train_ssm\n",
    "from diayn.examples.diayn import get_algorithm, experiment\n",
    "from rlkit.envs.wrappers import NormalizedBoxEnv\n",
    "from rlkit.samplers.util import DIAYNRollout as diayn_rollout\n",
    "from rlkit.samplers.util import rollout as random_rollout\n",
    "from rlkit.policies.simple import RandomPolicy\n",
    "\n",
    "\n",
    "def collect(env, diayn, depth, args):\n",
    "    if depth == 0:\n",
    "        random_policy = RandomPolicy(env.action_space)\n",
    "    else:\n",
    "        diayn_policy = diayn.eval_data_collector.get_snapshot()['policy']\n",
    "\n",
    "    data = []\n",
    "    for skill in tqdm(range(args.skill_dim)):\n",
    "        for trial in range(100):\n",
    "            # print(\"skill-{} rollout-{}\".format(skill, trial))\n",
    "            if depth == 0:\n",
    "                path = random_rollout(\n",
    "                    env,\n",
    "                    random_policy,\n",
    "                    max_path_length=args.H,\n",
    "                    render=False,\n",
    "                )\n",
    "            else:\n",
    "                path = diayn_rollout(\n",
    "                    env,\n",
    "                    diayn_policy,\n",
    "                    skill,\n",
    "                    max_path_length=args.H,\n",
    "                    render=False,\n",
    "                )\n",
    "            data.append([path['actions'], path['next_observations']])\n",
    "\n",
    "    train_data = data[:int(len(data)*0.9)]\n",
    "    test_data = data[int(len(data)*0.9):]\n",
    "\n",
    "    train_path = os.path.join(args.data_dir, \"./train{}.pkl\".format(depth))\n",
    "    test_path = os.path.join(args.data_dir, \"./test{}.pkl\".format(depth))\n",
    "\n",
    "    os.makedirs(args.data_dir, exist_ok=True)\n",
    "    with open(train_path, mode='wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(test_path, mode='wb') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "\n",
    "\n",
    "def update_sim(depth, args):\n",
    "    ssm, ssm_log = train_ssm(\"--H {} --depth {} --epochs 100\".format(args.H, depth))\n",
    "    sim = SimNormalizedBoxEnv(gym.make(str(args.env)), ssm, depth, args)\n",
    "    return sim, ssm_log\n",
    "\n",
    "\n",
    "def update_policy(diayn, sim, diayn_path, args):\n",
    "    experiment(diayn, sim, sim, args)\n",
    "    file = os.path.join(diayn_path, \"params.pkl\")\n",
    "    diayn, diayn_log = get_algorithm(env, env, args.skill_dim, file=file)\n",
    "    return diayn, diayn_log\n",
    "\n",
    "\n",
    "class SimNormalizedBoxEnv(NormalizedBoxEnv):\n",
    "    def __init__(self, env, ssm, depth, args):\n",
    "        super(SimNormalizedBoxEnv, self).__init__(env)\n",
    "        self.ssm = ssm\n",
    "        with open(os.path.join(args.data_dir, \"param{}.pkl\".format(depth)),\n",
    "                  mode='rb') as f:\n",
    "            self.a_mean, self.a_std, self.o_mean, self.o_std = \\\n",
    "                pickle.load(f)\n",
    "        self.env.step = self.step\n",
    "        self.envreset = self.env.reset\n",
    "        self.env.reset = self.reset\n",
    "\n",
    "    def step(self, action):\n",
    "        lb = self._wrapped_env.action_space.low\n",
    "        ub = self._wrapped_env.action_space.high\n",
    "        scaled_action = lb + (action + 1.) * 0.5 * (ub - lb)\n",
    "        scaled_action = np.clip(scaled_action, lb, ub)\n",
    "\n",
    "        # wrapped_step = self._wrapped_env.step(scaled_action)\n",
    "        # next_obs, reward, done, info = wrapped_step\n",
    "        # if self._should_normalize:\n",
    "        #     next_obs = self._apply_normalize_obs(next_obs)\n",
    "\n",
    "        a = scaled_action.astype(np.float32)\n",
    "        a = (a - self.a_mean) / self.a_std\n",
    "        a = torch.from_numpy(np.array([a]))\n",
    "        o = self.ssm.step(a)[0]\n",
    "        o = o.cpu().detach().numpy()\n",
    "        next_obs = o * self.o_std + self.o_mean\n",
    "\n",
    "        # return next_obs, reward * self._reward_scale, done, info\n",
    "        return next_obs, 0, False, {}\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        o_original = self.envreset(**kwargs)\n",
    "        o = o_original.astype(np.float32)\n",
    "        o = (o - self.o_mean) / self.o_std\n",
    "        o = torch.from_numpy(np.array([o]))\n",
    "        o = self.ssm.reset(o)\n",
    "        return o_original\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('env', type=str,\n",
    "                        help='environment')\n",
    "    parser.add_argument(\"--data_dir\", type=str,\n",
    "                        default=\"./data/\")\n",
    "    parser.add_argument('--skill_dim', type=int, default=100,\n",
    "                        help='skill dimension')\n",
    "    parser.add_argument('--H', type=int, default=300,\n",
    "                        help='Max length of rollout')\n",
    "    parser.add_argument('--D', type=int, default=5,\n",
    "                        help='Depth (The number of update)')\n",
    "    args = parser.parse_args(\"HalfCheetah-v2\".split())\n",
    "\n",
    "    env = NormalizedBoxEnv(gym.make(str(args.env)))\n",
    "    diayn, diayn_log = get_algorithm(env, env, args.skill_dim)\n",
    "\n",
    "    loghist = []\n",
    "    for depth in range(args.D):\n",
    "        collect(env, diayn, depth, args)\n",
    "        sim, ssm_log = update_sim(depth, args)\n",
    "        diayn, diayn_log = update_policy(diayn, sim, diayn_log, args)\n",
    "        loghist.append([ssm_log, diayn_log])\n",
    "        gt.reset()\n",
    "    print(loghist)\n",
    "    \n",
    "    with open(datetime.now().strftime(\"%b%d_%H-%M-%S\") + \"_loghist.txt\", mode='wb') as f:\n",
    "        pickle.dump(loghist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-venue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
